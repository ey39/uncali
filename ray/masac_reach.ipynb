{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78913d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from ray.tune.registry import register_env\n",
    "from common.envUtils import *\n",
    "\n",
    "\n",
    "class MultiAgentReachEnv(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.env_core = make_reach_her_env_masac(log_dir=config.get(\"log_dir\"))\n",
    "        \n",
    "        # multi agent id\n",
    "        self.possible_agents = ['agent_1', 'agent_2']\n",
    "        self.agents = self.possible_agents = ['agent_1', 'agent_2']\n",
    "\n",
    "        # multi agent obs_space & act_space\n",
    "        # self.observation_space = self.env_core.unwrapped.observation_spec\n",
    "        self.observation_spaces = {\n",
    "            \"agent_1\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "            \"agent_2\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32),\n",
    "        }\n",
    "        # self.action_space = self.env_core.unwrapped.action_spec\n",
    "        self.action_spaces = {\n",
    "            \"agent_1\": gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32),\n",
    "            \"agent_2\": gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32),\n",
    "        }\n",
    "    \n",
    "    def get_action_space(self, agent_id):\n",
    "        return self.action_spaces[agent_id]\n",
    "\n",
    "    def get_observation_space(self, agent_id):\n",
    "        return self.observation_spaces[agent_id]\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.env_core.reset()\n",
    "        return {\n",
    "            \"agent_1\": np.zeros(self.observation_spaces[\"agent_1\"].shape, dtype=self.observation_spaces[\"agent_1\"].dtype),\n",
    "            \"agent_2\": np.zeros(self.observation_spaces[\"agent_2\"].shape, dtype=self.observation_spaces[\"agent_2\"].dtype),\n",
    "        }, {'success': False}\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        action = np.concatenate([\n",
    "            action_dict[\"agent_1\"], \n",
    "            action_dict[\"agent_2\"],\n",
    "        ])\n",
    "        observation, reward, terminated, truncated, info = self.env_core.step(action)\n",
    "\n",
    "        observations = {\n",
    "            \"agent_1\": self.env_core._process_obs_her_masac(obs=observation, obs_type='pos'),\n",
    "            \"agent_2\": self.env_core._process_obs_her_masac(obs=observation, obs_type='rot'),\n",
    "        }\n",
    "\n",
    "        pos_err_reward, rot_err_reward, _, _, _ = self.env_core._get_reward_masac()\n",
    "        rewards = {\n",
    "            \"agent_1\": pos_err_reward + reward,\n",
    "            \"agent_2\": rot_err_reward + reward,\n",
    "        }\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/PosAgentReward\", rewards[\"agent_1\"])\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/RotAgentReward\", rewards[\"agent_2\"])\n",
    "\n",
    "        terminateds = {\n",
    "            \"agent_1\": terminated, \"agent_2\": terminated,\n",
    "        }\n",
    "\n",
    "        truncateds = {\n",
    "            \"agent_1\": truncated, \"agent_2\": truncated,\n",
    "        }\n",
    "        terminateds[\"__all__\"] = all(terminateds.values())\n",
    "        truncateds[\"__all__\"] = any(truncateds.values())\n",
    "\n",
    "        infos = {\n",
    "            \"agent_1\": info, \"agent_2\": info,\n",
    "        }\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "    \n",
    "register_env(\"MultiReach-v0\", MultiAgentReachEnv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "import numpy as np\n",
    "\n",
    "TASK=\"MultiAgentReach_\"\n",
    "experiment_name = TASK + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "LOGDIR=f\"/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/{experiment_name}\"\n",
    "\n",
    "def policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    if agent_id == \"agent_1\":\n",
    "        p = \"policy_1\"\n",
    "    elif agent_id == \"agent_2\":\n",
    "        p = \"policy_2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "    # print(f\"[mapping] {agent_id} -> {p}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\n",
    "        env=\"MultiReach-v0\",\n",
    "        env_config={\"log_dir\": LOGDIR},        \n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_1\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "            \"policy_2\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_1\", \"policy_2\",]\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={\n",
    "                \"policy_1\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "                \"policy_2\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .training(\n",
    "        initial_alpha=0.2,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-4,\n",
    "        alpha_lr=1e-4,\n",
    "        target_entropy=\"auto\",\n",
    "        n_step=1,\n",
    "        tau=0.005,\n",
    "        train_batch_size=128,\n",
    "        target_network_update_freq=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentEpisodeReplayBuffer\",\n",
    "            \"capacity\": 1000000,\n",
    "            \"learning_starts\": 1000,\n",
    "        },\n",
    "        num_steps_sampled_before_learning_starts=1000,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [512, 512],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"post_fcnet_weights_initializer\": \"orthogonal_\",\n",
    "            \"post_fcnet_weights_initializer_config\": {\"gain\": 0.01},\n",
    "        },\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0.25,      # 或 0.25 视机器配置\n",
    "        num_cpus_per_worker=1,\n",
    "        num_learner_workers=1,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=5,\n",
    "        min_sample_timesteps_per_iteration=1000,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_num_env_runners=1,\n",
    "        evaluation_config={\"seed\": 42},\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=6,             # 进程数量\n",
    "        num_envs_per_env_runner=1,     # 环境数量\n",
    "        # gym_env_vectorize_mode=\"ASYNC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "from ray import train, tune, air\n",
    "tuner = tune.Tuner(\n",
    "    trainable=config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"multi_agent_reach\",\n",
    "        storage_path=LOGDIR,\n",
    "        log_to_file=True,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=10,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "        stop={\"evaluation/env_runners/episode_return_mean\": 18000.0}\n",
    "    )\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceedbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spec1 = RLModuleSpec(\n",
    "#     module_class=rl_module.__class__,\n",
    "#     observation_space=rl_module.observation_space,\n",
    "#     action_space=rl_module.action_space,\n",
    "#     model_config={\"twin_q\": True},\n",
    "#     load_state_path=checkpoint_path \n",
    "# )\n",
    "\n",
    "from ray.rllib.policy.policy import Policy\n",
    "checkpoint_path = \"/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/Reach_2025-08-19_20-10-32/reach/SAC_ReachEnvHERGym_81d54_00000_0_2025-08-19_20-10-33/checkpoint_000155/learner_group/learner/rl_module/default_policy\"\n",
    "my_restored_policy = Policy.from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef28a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758873327.312558   26380 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758873327.315941   26380 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1758873327.324664   26380 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758873327.324679   26380 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758873327.324681   26380 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1758873327.324682   26380 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from ray.tune.registry import register_env\n",
    "from common.envUtils import *\n",
    "import random\n",
    "\n",
    "\n",
    "class MultiAgentReachEnv(MultiAgentEnv):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        self.env_core = make_reach_her_env_masac(log_dir=config.get(\"log_dir\"), has_renderer=True)\n",
    "        \n",
    "        # multi agent id\n",
    "        self.possible_agents = ['agent_1', 'agent_2']\n",
    "        self.agents = self.possible_agents = ['agent_1', 'agent_2']\n",
    "\n",
    "        # multi agent obs_space & act_space\n",
    "        self.observation_spaces = {\n",
    "            \"agent_1\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "            \"agent_2\": gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "        }\n",
    "        self.action_spaces = {\n",
    "            \"agent_1\": gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "            \"agent_2\": gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "        }\n",
    "    \n",
    "    def get_action_space(self, agent_id):\n",
    "        return self.action_spaces[agent_id]\n",
    "\n",
    "    def get_observation_space(self, agent_id):\n",
    "        return self.observation_spaces[agent_id]\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.env_core.reset()\n",
    "        return {\n",
    "            \"agent_1\": np.zeros(self.observation_spaces[\"agent_1\"].shape, dtype=self.observation_spaces[\"agent_1\"].dtype),\n",
    "            \"agent_2\": np.zeros(self.observation_spaces[\"agent_2\"].shape, dtype=self.observation_spaces[\"agent_2\"].dtype),\n",
    "        }, {'success': False}\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        action = np.concatenate([\n",
    "            action_dict[\"agent_1\"], \n",
    "            action_dict[\"agent_2\"],\n",
    "        ])\n",
    "        observation, reward, terminated, truncated, info = self.env_core.step(action)\n",
    "        pos_err_reward, rot_err_reward, pose_err_reward, action_penalty, vel_penalty = self.env_core._get_reward_masac()\n",
    "\n",
    "        observations = {\n",
    "            \"agent_1\": self.env_core._process_obs_masac(obs=observation, obs_type='pos'),\n",
    "            \"agent_2\": self.env_core._process_obs_masac(obs=observation, obs_type='rot'),\n",
    "        }\n",
    "        rewards = {\n",
    "            \"agent_1\": pos_err_reward + pose_err_reward + action_penalty + vel_penalty,\n",
    "            \"agent_2\": rot_err_reward + pose_err_reward + action_penalty + vel_penalty,\n",
    "        }\n",
    "\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/EnvOriginReward\", reward)\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/PosProcessReward\", pos_err_reward)\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/RotProcessReward\", rot_err_reward)\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/PosAgentReward\", rewards[\"agent_1\"])\n",
    "        self.env_core.unwrapped.write_tensorboard(\"Reward/RotAgentReward\", rewards[\"agent_2\"])\n",
    "\n",
    "        terminateds = {\n",
    "            \"agent_1\": terminated, \"agent_2\": terminated,\n",
    "        }\n",
    "\n",
    "        truncateds = {\n",
    "            \"agent_1\": truncated, \"agent_2\": truncated,\n",
    "        }\n",
    "        terminateds[\"__all__\"] = all(terminateds.values())\n",
    "        truncateds[\"__all__\"] = any(truncateds.values())\n",
    "\n",
    "        infos = {\n",
    "            \"agent_1\": info, \"agent_2\": info,\n",
    "        }\n",
    "\n",
    "        return observations, rewards, terminateds, truncateds, infos\n",
    "    \n",
    "register_env(\"MultiReach-v0\", MultiAgentReachEnv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafecbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "TASK=\"MultiAgentReach_\"\n",
    "experiment_name = TASK + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "LOGDIR=f\"/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/{experiment_name}\"\n",
    "\n",
    "def policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    if agent_id == \"agent_1\":\n",
    "        p = \"policy_1\"\n",
    "    elif agent_id == \"agent_2\":\n",
    "        p = \"policy_2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "    # print(f\"[mapping] {agent_id} -> {p}\")\n",
    "    return p\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\n",
    "        env=\"MultiReach-v0\",\n",
    "        env_config={\"log_dir\": LOGDIR},        \n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_1\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "            \"policy_2\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_1\", \"policy_2\",]\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={\n",
    "                \"policy_1\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "                \"policy_2\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .training(\n",
    "        twin_q=True,\n",
    "        initial_alpha=0.2,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-4,\n",
    "        alpha_lr=1e-4,\n",
    "        target_entropy=\"auto\",\n",
    "        n_step=1,\n",
    "        tau=0.005,\n",
    "        train_batch_size=128,\n",
    "        target_network_update_freq=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentEpisodeReplayBuffer\",\n",
    "            \"capacity\": 1000000,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"replay_batch_size\": 200,\n",
    "        },\n",
    "        num_steps_sampled_before_learning_starts=1000,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [512, 512],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"post_fcnet_weights_initializer\": \"orthogonal_\",\n",
    "            \"post_fcnet_weights_initializer_config\": {\"gain\": 0.01},\n",
    "        },\n",
    "    )\n",
    "    .resources(\n",
    "        # num_cpus=10,\n",
    "    #     num_gpus=0.25,      # 或 0.25 视机器配置\n",
    "        num_cpus_per_worker=10,\n",
    "    #     num_learner_workers=1,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=5,\n",
    "        min_sample_timesteps_per_iteration=1000,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_num_env_runners=1,\n",
    "        evaluation_duration=2,\n",
    "        evaluation_config={\"seed\": 42},\n",
    "    )\n",
    "    .env_runners(\n",
    "        rollout_fragment_length=200,\n",
    "    )\n",
    "    # .env_runners(\n",
    "    #     num_env_runners=6,             # 进程数量\n",
    "    #     num_envs_per_env_runner=1,     # 环境数量\n",
    "    #     # gym_env_vectorize_mode=\"ASYNC\"\n",
    "    # )\n",
    ")\n",
    "\n",
    "from ray import train, tune, air\n",
    "\n",
    "def _on_sample_end(env_runner, metrics_logger, samples, **kwargs):\n",
    "    if not hasattr(_on_sample_end, \"count\"):\n",
    "        _on_sample_end.count = 0  # 初始化静态变量\n",
    "    agent_1_episode = samples[0].agent_episodes[\"agent_1\"]\n",
    "    agent_2_episode = samples[0].agent_episodes[\"agent_2\"]\n",
    "    # print(f\"samples len:{len(samples)}\")\n",
    "    if len(agent_1_episode) == 200:\n",
    "        if (random.random() < 0.1):\n",
    "            err = {\n",
    "                \"pos_init\": calculate_pos_error(\n",
    "                    np.array(agent_1_episode.observations[1][:3]), \n",
    "                    np.array(agent_1_episode.observations[1][-3:])\n",
    "                ),\n",
    "                \"rot_init\": calculate_rot_error(\n",
    "                    np.array(agent_2_episode.observations[1][:3]), \n",
    "                    np.array(agent_2_episode.observations[1][-3:]),\n",
    "                    angle_unit='radians'\n",
    "                ),\n",
    "            }\n",
    "        # pos\n",
    "            for i in range(len(agent_1_episode.observations)):\n",
    "                agent_1_episode.observations[i][:3] = agent_1_episode.observations[-1][-3:]\n",
    "                err[\"pos\"] = calculate_pos_error(\n",
    "                    np.array(agent_1_episode.observations[i][:3]), \n",
    "                    np.array(agent_1_episode.observations[i][-3:])\n",
    "                )\n",
    "                agent_1_episode.rewards[i-1] = ReachEnv.cal_pos_reward(error=err)\n",
    "        # rot\n",
    "            # for i in range(len(agent_2_episode.observations))[1:]:\n",
    "                agent_2_episode.observations[i][:3] = agent_2_episode.observations[-1][-3:]\n",
    "                err[\"rot\"] = calculate_rot_error(\n",
    "                    np.array(agent_2_episode.observations[i][:3]), \n",
    "                    np.array(agent_2_episode.observations[i][-3:]),\n",
    "                    angle_unit='radians'\n",
    "                )\n",
    "                agent_2_episode.rewards[i-1] = ReachEnv.cal_rot_reward(error=err)\n",
    "    # for i in range(len(ep1.observations)):\n",
    "    #     if ep1.observations[i][0] == 0 and  ep1.observations[i][1] == 0:\n",
    "    #         print(_on_sample_end.count)\n",
    "    #         _on_sample_end.count = 0\n",
    "    #     else:\n",
    "    #         _on_sample_end.count += 1\n",
    "    #     print(f\"ep1 obs:{ep1.observations[i]}\")\n",
    "    # for i in range(len(ep1.rewards)):\n",
    "    #     print(f\"ep1 r:{ep1.rewards[i]}\")\n",
    "    # for i in range(len(ep1.actions)):\n",
    "    #     print(f\"ep1 act:{ep1.actions[i]}\")\n",
    "\n",
    "    \n",
    "config.callbacks(on_sample_end=_on_sample_end)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"multi_agent_reach\",\n",
    "        storage_path=LOGDIR,\n",
    "        log_to_file=True,\n",
    "        checkpoint_config=air.CheckpointConfig(\n",
    "            checkpoint_frequency=10,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "        stop={\"evaluation/env_runners/episode_return_mean\": 1800000.0},\n",
    "        # callbacks=[MyCheckpointCallback()],  # 挂上去\n",
    "    )\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34727195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-09-26 15:56:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:34.75        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.4/15.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 11.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  num_training_step_ca\n",
       "lls_per_iteration</th><th style=\"text-align: right;\">          num_env_steps_sample\n",
       "d_lifetime</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_MultiReach-v0_2e4bc_00000</td><td>TERMINATED</td><td>127.0.1.1:27559</td><td style=\"text-align: right;\">  3971</td><td style=\"text-align: right;\">         20.2729</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">3.971e+06</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 15:55:34,047\tWARNING algorithm_config.py:5045 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-09-26 15:55:34,049\tWARNING algorithm_config.py:5074 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2025-09-26 15:55:34,049\tWARNING sac.py:489 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-09-26 15:55:34,051\tWARNING algorithm_config.py:5074 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "2025-09-26 15:55:34,051\tWARNING sac.py:489 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(pid=27559)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=27559)\u001b[0m E0000 00:00:1758873334.884975   27559 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=27559)\u001b[0m E0000 00:00:1758873334.888397   27559 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=27559)\u001b[0m W0000 00:00:1758873334.897299   27559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27559)\u001b[0m W0000 00:00:1758873334.897324   27559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27559)\u001b[0m W0000 00:00:1758873334.897327   27559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27559)\u001b[0m W0000 00:00:1758873334.897328   27559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:38,622\tWARNING algorithm_config.py:5045 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:38,623\tWARNING algorithm_config.py:5074 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:38,623\tWARNING sac.py:489 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SAC pid=27559)\u001b[0m [chatbus_6] 共享内存不存在，创建成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SAC pid=27559)\u001b[0m /home/ey/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m /home/ey/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:40,892\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:40,906\tWARNING algorithm_config.py:5074 -- You configured a custom `model` config (probably through calling config.training(model=..), whereas your config uses the new API stack! In order to switch off the new API stack, set in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. If you DO want to use the new API stack, configure your model, instead, through: `config.rl_module(model_config={..})`.\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m 2025-09-26 15:55:40,907\tWARNING sac.py:489 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m [2025-09-26 15:55:40,935 E 27559 27559] core_worker.cc:2246: Actor with class name: 'MultiAgentEnvRunner' and ID: 'cb444968e3d7d3f43cce5bbb01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(pid=27698)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=27698)\u001b[0m E0000 00:00:1758873341.660322   27698 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=27698)\u001b[0m E0000 00:00:1758873341.663692   27698 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=27698)\u001b[0m W0000 00:00:1758873341.672351   27698 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27698)\u001b[0m W0000 00:00:1758873341.672371   27698 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27698)\u001b[0m W0000 00:00:1758873341.672373   27698 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=27698)\u001b[0m W0000 00:00:1758873341.672374   27698 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-09-26 15:55:43,344 E 26589 26614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-09-26_15-55-32_394818_26380 is over 95% full, available space: 18.716 GB; capacity: 467.89 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m [chatbus_7] 共享内存不存在，创建成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m /home/ey/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m /home/ey/.local/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=27698)\u001b[0m 2025-09-26 15:55:47,803\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SAC pid=27559)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-09-26 15:55:53,348 E 26589 26614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-09-26_15-55-32_394818_26380 is over 95% full, available space: 18.7155 GB; capacity: 467.89 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-09-26 15:56:03,353 E 26589 26614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-09-26_15-55-32_394818_26380 is over 95% full, available space: 18.7146 GB; capacity: 467.89 GB. Object creation will fail if spilling is required.\n",
      "2025-09-26 15:56:08,789\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/MultiAgentReach_2025-09-26_15-55-32/multi_agent_reach' in 0.0206s.\n",
      "\u001b[36m(SAC(env=MultiReach-v0; env-runners=0; learners=0; multi-agent=True) pid=27559)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/MultiAgentReach_2025-09-26_15-55-32/multi_agent_reach/SAC_MultiReach-v0_2e4bc_00000_0_2025-09-26_15-55-34/checkpoint_000000)\n",
      "2025-09-26 15:56:09,189\tINFO tune.py:1041 -- Total run time: 35.17 seconds (34.72 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-09-26 15:56:13,368 E 26589 26614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-09-26_15-55-32_394818_26380 is over 95% full, available space: 18.7069 GB; capacity: 467.89 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-09-26 15:56:23,381 E 26589 26614] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-09-26_15-55-32_394818_26380 is over 95% full, available space: 18.7065 GB; capacity: 467.89 GB. Object creation will fail if spilling is required.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "TASK=\"MultiAgentReach_\"\n",
    "experiment_name = TASK + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "LOGDIR=f\"/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/{experiment_name}\"\n",
    "\n",
    "def policy_mapping_fn(agent_id, *args, **kwargs):\n",
    "    if agent_id == \"agent_1\":\n",
    "        p = \"policy_1\"\n",
    "    elif agent_id == \"agent_2\":\n",
    "        p = \"policy_2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent_id: {agent_id}\")\n",
    "    # print(f\"[mapping] {agent_id} -> {p}\")\n",
    "    return p\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\n",
    "        env=\"MultiReach-v0\",\n",
    "        env_config={\"log_dir\": LOGDIR},        \n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"policy_1\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "            \"policy_2\": (None, gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32), gym.spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), {}),\n",
    "        },\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"policy_1\", \"policy_2\",]\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={\n",
    "                \"policy_1\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "                \"policy_2\": RLModuleSpec(\n",
    "                    observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32),\n",
    "                    action_space = gym.spaces.Box(low=-0.5, high=0.5, shape=(3,), dtype=np.float32),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .training(\n",
    "        twin_q=True,\n",
    "        initial_alpha=0.2,\n",
    "        actor_lr=1e-4,\n",
    "        critic_lr=1e-4,\n",
    "        alpha_lr=1e-4,\n",
    "        target_entropy=\"auto\",\n",
    "        n_step=1,\n",
    "        tau=0.005,\n",
    "        train_batch_size=128,\n",
    "        target_network_update_freq=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentEpisodeReplayBuffer\",\n",
    "            \"capacity\": 1000000,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"replay_batch_size\": 200,\n",
    "        },\n",
    "        num_steps_sampled_before_learning_starts=1000,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [512, 512],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"post_fcnet_weights_initializer\": \"orthogonal_\",\n",
    "            \"post_fcnet_weights_initializer_config\": {\"gain\": 0.01},\n",
    "        },\n",
    "    )\n",
    "    .resources(\n",
    "        # num_cpus=10,\n",
    "    #     num_gpus=0.25,      # 或 0.25 视机器配置\n",
    "        num_cpus_per_worker=10,\n",
    "    #     num_learner_workers=1,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=5,\n",
    "        min_sample_timesteps_per_iteration=1000,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_num_env_runners=1,\n",
    "        evaluation_duration=2,\n",
    "        evaluation_config={\"seed\": 42},\n",
    "    )\n",
    "    .env_runners(\n",
    "        rollout_fragment_length=200,\n",
    "    )\n",
    "    # .env_runners(\n",
    "    #     num_env_runners=6,             # 进程数量\n",
    "    #     num_envs_per_env_runner=1,     # 环境数量\n",
    "    #     # gym_env_vectorize_mode=\"ASYNC\"\n",
    "    # )\n",
    ")\n",
    "checkpoint_dir = \"/home/ey/rl/src/rlreach2/rlreach/ray/db/ray_results/MultiAgentReach_2025-09-25_17-43-16/multi_agent_reach/SAC_MultiReach-v0_11058_00000_0_2025-09-25_17-43-18/checkpoint_000396\"\n",
    "config.callbacks(\n",
    "    on_algorithm_init=(\n",
    "        lambda algorithm, _dir=checkpoint_dir, **kw: algorithm.restore_from_path(_dir)\n",
    "    ),\n",
    ")\n",
    "\n",
    "from ray import train, tune, air\n",
    "\n",
    "results = tune.Tuner(\n",
    "    trainable=config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"multi_agent_reach\",\n",
    "        storage_path=LOGDIR,\n",
    "        log_to_file=True,\n",
    "        # stop={\"num_env_steps_sampled_lifetime\": 80000},\n",
    "        # callbacks=[MyCheckpointCallback()],  # 挂上去\n",
    "    )\n",
    ").fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlreach310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
