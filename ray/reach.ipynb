{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6966584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753340618.310483   45727 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753340618.313951   45727 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753340618.323201   45727 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753340618.323216   45727 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753340618.323218   45727 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753340618.323219   45727 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.env.multi_agent_env import make_multi_agent\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from robosuite.wrappers import GymWrapper\n",
    "from common.reachEnv import * \n",
    "from common.envUtils import *\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "# register(\n",
    "#     id=\"ReachEnvUR5e-v0\",\n",
    "#     entry_point=\"common.envUtils:ReachEnvGym\",  # 替换成你的模块路径\n",
    "#     kwargs={'env_config' : None},\n",
    "# )\n",
    "\n",
    "# MultiAgentReach = make_multi_agent(\"ReachEnvUR5e-v0\")\n",
    "\n",
    "# register_env(\n",
    "#     \"env\",\n",
    "#     lambda _: MultiAgentReach(config={\"num_agents\": 2}),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c5de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753341092.007975    5915 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753341092.022927    5915 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753341092.132423    5915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753341092.132443    5915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753341092.132445    5915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753341092.132446    5915 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "2025-07-24 15:11:37,126\tWARNING algorithm_config.py:5033 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-07-24 15:11:37,134\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-07-24 15:11:38,250\tINFO worker.py:1927 -- Started a local Ray instance.\n",
      "[2025-07-24 15:11:38,868 E 5915 5915] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'f6a7a26897191442c9eb6b0b01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-07-24 15:11:38,887 E 5915 5915] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '41b815efd8919e774e20f95101000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-07-24 15:11:38,906 E 5915 5915] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '06a29fae74cd53c69854127901000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-07-24 15:11:38,925 E 5915 5915] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '26fae8f5e3c9d254a846514701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(pid=9504)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=9504)\u001b[0m E0000 00:00:1753341099.154712    9504 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=9504)\u001b[0m E0000 00:00:1753341099.158223    9504 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=9504)\u001b[0m W0000 00:00:1753341099.167171    9504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=9504)\u001b[0m W0000 00:00:1753341099.167196    9504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=9504)\u001b[0m W0000 00:00:1753341099.167198    9504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=9504)\u001b[0m W0000 00:00:1753341099.167199    9504 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/chatbus_1'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self.make_env()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 928, in make_vec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     env = gym.vector.AsyncVectorEnv(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 144, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     dummy_env = env_fns[0]()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 118, in _gym_env_creator\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     env = env_descriptor(env_context)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 77, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self.env = make_reach_env()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 71, in make_reach_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     env = ReachEnv(**reach_env_config)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/reachEnv.py\", line 98, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self.channel = SharedMemoryChannel(f\"chatbus_{n_env}\")   #\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/shmUtils.py\", line 17, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self.shm = shared_memory.SharedMemory(name=name, create=True, size=size)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9504)\u001b[0m FileExistsError: [Errno 17] File exists: '/chatbus_1'\n",
      "\u001b[36m(pid=9510)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=9510)\u001b[0m E0000 00:00:1753341099.205194    9510 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=9510)\u001b[0m E0000 00:00:1753341099.208810    9510 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=9510)\u001b[0m W0000 00:00:1753341099.218268    9510 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=9502)\u001b[0m [chatbus_1] 共享内存不存在，创建成功\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m [chatbus_1] 共享内存已存在，连接成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9502)\u001b[0m 2025-07-24 15:11:45,992\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-07-24 15:11:46,116\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "    self._fd = _posixshmem.shm_open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/chatbus_1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 928, in make_vec\n",
      "    env = gym.vector.AsyncVectorEnv(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 144, in __init__\n",
      "    dummy_env = env_fns[0]()\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 118, in _gym_env_creator\n",
      "    env = env_descriptor(env_context)\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 77, in __init__\n",
      "    self.env = make_reach_env()\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 71, in make_reach_env\n",
      "    env = ReachEnv(**reach_env_config)\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/reachEnv.py\", line 98, in __init__\n",
      "    self.channel = SharedMemoryChannel(f\"chatbus_{n_env}\")   #\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/shmUtils.py\", line 17, in __init__\n",
      "    self.shm = shared_memory.SharedMemory(name=name, create=True, size=size)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "    self._fd = _posixshmem.shm_open(\n",
      "FileExistsError: [Errno 17] File exists: '/chatbus_1'), taking actor 2 out of service.\n",
      "2025-07-24 15:11:46,117\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "    self._fd = _posixshmem.shm_open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/chatbus_1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=9504, ip=192.168.202.162, actor_id=41b815efd8919e774e20f95101000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7e77234f0340>)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 928, in make_vec\n",
      "    env = gym.vector.AsyncVectorEnv(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 144, in __init__\n",
      "    dummy_env = env_fns[0]()\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 118, in _gym_env_creator\n",
      "    env = env_descriptor(env_context)\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 77, in __init__\n",
      "    self.env = make_reach_env()\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 71, in make_reach_env\n",
      "    env = ReachEnv(**reach_env_config)\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/reachEnv.py\", line 98, in __init__\n",
      "    self.channel = SharedMemoryChannel(f\"chatbus_{n_env}\")   #\n",
      "  File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/shmUtils.py\", line 17, in __init__\n",
      "    self.shm = shared_memory.SharedMemory(name=name, create=True, size=size)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "    self._fd = _posixshmem.shm_open(\n",
      "FileExistsError: [Errno 17] File exists: '/chatbus_1'\n",
      "2025-07-24 15:11:46,332\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-07-24 15:11:46,372\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "[2025-07-24 15:11:46,396 E 5915 5915] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'e3a6df6f4a244339e739f50e01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9503)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9503)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9503)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9507)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=9503)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=9503)\u001b[0m E0000 00:00:1753341106.685301    9503 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=9503)\u001b[0m E0000 00:00:1753341106.688792    9503 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=9503)\u001b[0m W0000 00:00:1753341106.697862    9503 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m   gym.logger.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=9510)\u001b[0m 2025-07-24 15:11:46,093\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=9503)\u001b[0m [chatbus_1] 共享内存已存在，连接成功\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 15:11:54,711\tINFO trainable.py:161 -- Trainable.setup took 17.566 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2025-07-24 15:11:54,712\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=10715)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=10715)\u001b[0m E0000 00:00:1753341125.919335   10715 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=10715)\u001b[0m E0000 00:00:1753341125.922704   10715 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=10715)\u001b[0m W0000 00:00:1753341125.931920   10715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10715)\u001b[0m W0000 00:00:1753341125.931946   10715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10715)\u001b[0m W0000 00:00:1753341125.931947   10715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10715)\u001b[0m W0000 00:00:1753341125.931948   10715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:10,761\tWARNING algorithm_config.py:5033 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:10,769\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m [2025-07-24 15:12:11,051 E 10715 10715] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'bfdc620e7a74ec571cf3129701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m [2025-07-24 15:12:11,069 E 10715 10715] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'a9ff669fecbd28473d9ae30801000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m [2025-07-24 15:12:11,088 E 10715 10715] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'a5b9d22ec0d917a965ccbcdc01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m [2025-07-24 15:12:11,106 E 10715 10715] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '4c4b22b915150f292e1d82ae01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(pid=10790)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(pid=10790)\u001b[0m E0000 00:00:1753341131.899083   10790 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=10790)\u001b[0m E0000 00:00:1753341131.902553   10790 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=10790)\u001b[0m W0000 00:00:1753341131.911468   10790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10790)\u001b[0m W0000 00:00:1753341131.911497   10790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10790)\u001b[0m W0000 00:00:1753341131.911498   10790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(pid=10790)\u001b[0m W0000 00:00:1753341131.911499   10790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=10789)\u001b[0m [chatbus_1] 共享内存已存在，连接成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m   gym.logger.warn(\n",
      "\u001b[36m(pid=10788)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=10788)\u001b[0m E0000 00:00:1753341131.957872   10788 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=10788)\u001b[0m E0000 00:00:1753341131.961324   10788 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=10788)\u001b[0m W0000 00:00:1753341131.970148   10788 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m 2025-07-24 15:12:20,218\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:20,418\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m [2025-07-24 15:12:20,441 E 10715 10715] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '4d5d1612f1540fcf9505e8af01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Process Worker<AsyncVectorEnv>-1:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 701, in _async_worker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     command, data = pipe.recv()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     buf = self._recv_bytes()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 383, in _recv\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     raise EOFError\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m EOFError\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self.run()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 779, in _async_worker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     pipe.send((None, False))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 206, in send\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._send(header + buf)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     n = write(self._handle, buf)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m BrokenPipeError: [Errno 32] Broken pipe\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Process Worker<AsyncVectorEnv>-0:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 701, in _async_worker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     command, data = pipe.recv()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     buf = self._recv_bytes()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     buf = self._recv(4)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 383, in _recv\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     raise EOFError\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m EOFError\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self.run()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 779, in _async_worker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     pipe.send((None, False))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 206, in send\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     self._send(header + buf)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m     n = write(self._handle, buf)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m BrokenPipeError: [Errno 32] Broken pipe\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10787)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10788)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10790)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10788)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10788)\u001b[0m   gym.logger.warn(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10788)\u001b[0m /home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=11176)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=11176)\u001b[0m E0000 00:00:1753341142.243842   11176 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=11176)\u001b[0m E0000 00:00:1753341142.247264   11176 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(pid=11176)\u001b[0m W0000 00:00:1753341142.256667   11176 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:20,400\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:57)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:58)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/ey/rl/src/robosuite/robosuite/scripts/setup_macros.py (macros.py:59)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not import robosuite_models. Some robots may not be available. If you want to use these robots, please install robosuite_models from source (https://github.com/ARISE-Initiative/robosuite_models) or through pip install. (__init__.py:30)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mCould not load the mink-based whole-body IK. Make sure you install related import properly, otherwise you will not be able to use the default IK controller setting for GR1 robot. (__init__.py:40)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m \u001b[1m\u001b[32m[robosuite INFO] \u001b[0mLoading controller configuration from: ./common/reachController.json (composite_controller_factory.py:121)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:29,301\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=11176, ip=192.168.202.162, actor_id=4d5d1612f1540fcf9505e8af01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72956d5242e0>)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/chatbus_1'\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \n",
      "\u001b[36m(SAC pid=10715)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=11176, ip=192.168.202.162, actor_id=4d5d1612f1540fcf9505e8af01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72956d5242e0>)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.make_env()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 928, in make_vec\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = gym.vector.AsyncVectorEnv(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 144, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     dummy_env = env_fns[0]()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 118, in _gym_env_creator\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = env_descriptor(env_context)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 77, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.env = make_reach_env()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 71, in make_reach_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = ReachEnv(**reach_env_config)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/reachEnv.py\", line 98, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.channel = SharedMemoryChannel(f\"chatbus_{n_env}\")   #\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/shmUtils.py\", line 17, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.shm = shared_memory.SharedMemory(name=name, create=True, size=size)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m FileExistsError: [Errno 17] File exists: '/chatbus_1'), taking actor 1 out of service.\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m 2025-07-24 15:12:29,301\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=11176, ip=192.168.202.162, actor_id=4d5d1612f1540fcf9505e8af01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72956d5242e0>)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/chatbus_1'\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \n",
      "\u001b[36m(SAC pid=10715)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \n",
      "\u001b[36m(SAC pid=10715)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=11176, ip=192.168.202.162, actor_id=4d5d1612f1540fcf9505e8af01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72956d5242e0>)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.make_env()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 928, in make_vec\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = gym.vector.AsyncVectorEnv(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/vector/async_vector_env.py\", line 144, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     dummy_env = env_fns[0]()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 118, in _gym_env_creator\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = env_descriptor(env_context)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 77, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.env = make_reach_env()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/envUtils.py\", line 71, in make_reach_env\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     env = ReachEnv(**reach_env_config)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/reachEnv.py\", line 98, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.channel = SharedMemoryChannel(f\"chatbus_{n_env}\")   #\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/rl/src/rlreach2/rlreach/ray/common/shmUtils.py\", line 17, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.shm = shared_memory.SharedMemory(name=name, create=True, size=size)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/multiprocessing/shared_memory.py\", line 104, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self._fd = _posixshmem.shm_open(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m FileExistsError: [Errno 17] File exists: '/chatbus_1'\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SAC.__init__()\u001b[39m (pid=10715, ip=192.168.202.162, actor_id=bbaf7277a012406440c3dfb801000000, repr=SAC(env=<class 'common.envUtils.ReachEnvGym'>; env-runners=4; learners=0; multi-agent=False))\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/sac/sac.py\", line 570, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     super().__init__(*args, **kwargs)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 536, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     super().__init__(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py\", line 611, in setup\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     super().setup(config)\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 677, in setup\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self.eval_env_runner_group: EnvRunnerGroup = EnvRunnerGroup(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     self._setup(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 286, in _setup\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     spaces = self.get_spaces()\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m   File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 314, in get_spaces\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m     spaces = self.foreach_env_runner(\n",
      "\u001b[36m(SAC pid=10715)\u001b[0m IndexError: list index out of range\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=11176, ip=192.168.202.162, actor_id=4d5d1612f1540fcf9505e8af01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72956d5242e0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=11176)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=11078)\u001b[0m [chatbus_1] 共享内存不存在，创建成功\n",
      "\u001b[36m(SingleAgentEnvRunner pid=10788)\u001b[0m [chatbus_1] 共享内存已存在，连接成功\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-07-24 15:12:38,172 E 9425 9425] (raylet) node_manager.cc:3041: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 5e60799a1329c996a6e1790081232bfc71dafd4fd7f781ee77d682eb, IP: 192.168.202.162) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.202.162`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from ray import train, tune\n",
    "import ray\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from common.envUtils import *\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(env=ReachEnvGym)\n",
    "    .training(\n",
    "        initial_alpha=1.001,\n",
    "        actor_lr=3e-5,\n",
    "        critic_lr=3e-4,\n",
    "        alpha_lr=1e-4,\n",
    "        target_entropy=\"auto\",\n",
    "        n_step=1,\n",
    "        tau=0.005,\n",
    "        train_batch_size=256,\n",
    "        target_network_update_freq=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"PrioritizedEpisodeReplayBuffer\",\n",
    "            \"capacity\": 1000000,\n",
    "            \"alpha\": 0.6,\n",
    "            \"beta\": 0.4,\n",
    "        },\n",
    "        num_steps_sampled_before_learning_starts=256,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "            \"post_fcnet_hiddens\": [],\n",
    "            \"post_fcnet_activation\": None,\n",
    "            \"post_fcnet_weights_initializer\": \"orthogonal_\",\n",
    "            \"post_fcnet_weights_initializer_config\": {\"gain\": 0.01},\n",
    "        },\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    # .rl_module(\n",
    "    #     model_config=DefaultModelConfig(\n",
    "    #         fcnet_hiddens=[1024, 1024],\n",
    "    #         fcnet_activation=\"relu\",\n",
    "    #         fcnet_kernel_initializer=nn.init.xavier_uniform_,\n",
    "    #         head_fcnet_hiddens=[],\n",
    "    #         head_fcnet_activation=None,\n",
    "    #         head_fcnet_kernel_initializer=\"orthogonal_\",\n",
    "    #         head_fcnet_kernel_initializer_kwargs={\"gain\": 0.01},\n",
    "    #     )\n",
    "    # )\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=5,\n",
    "        min_sample_timesteps_per_iteration=1000,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "        evaluation_num_env_runners=1,\n",
    "        evaluation_config={\"seed\": 42},\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=4,             # Number of parallel processes (<= CPUs)\n",
    "        num_envs_per_env_runner=2,     # Vectorized environments per process\n",
    "        gym_env_vectorize_mode=\"ASYNC\"\n",
    "    )\n",
    ")\n",
    "# Build the Algorithm (PPO).\n",
    "sac = config.build_algo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0bfc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-24 15:12:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:24.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.0/15.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_ReachEnvGym_80b15_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-07-24_15-11-37_434713_5915/artifacts/2025-07-24_15-12-04/SAC_2025-07-24_15-12-04/driver_artifacts/SAC_ReachEnvGym_80b15_00000_0_2025-07-24_15-12-04/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SAC_ReachEnvGym_80b15_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 15:12:04,893\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-07-24 15:12:04,894\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-07-24 15:12:29,395\tERROR tune_controller.py:1331 -- Trial task failed for trial SAC_ReachEnvGym_80b15_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/_private/worker.py\", line 2858, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/_private/worker.py\", line 960, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::SAC.__init__()\u001b[39m (pid=10715, ip=192.168.202.162, actor_id=bbaf7277a012406440c3dfb801000000, repr=SAC(env=<class 'common.envUtils.ReachEnvGym'>; env-runners=4; learners=0; multi-agent=False))\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/sac/sac.py\", line 570, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 536, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn.py\", line 611, in setup\n",
      "    super().setup(config)\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 677, in setup\n",
      "    self.eval_env_runner_group: EnvRunnerGroup = EnvRunnerGroup(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 286, in _setup\n",
      "    spaces = self.get_spaces()\n",
      "  File \"/home/ey/anaconda3/envs/rlreach310/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 314, in get_spaces\n",
      "    spaces = self.foreach_env_runner(\n",
      "IndexError: list index out of range\n",
      "2025-07-24 15:12:29,412\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/ey/ray_results/SAC_2025-07-24_15-12-04' in 0.0057s.\n",
      "2025-07-24 15:12:29,418\tERROR tune.py:1037 -- Trials did not complete: [SAC_ReachEnvGym_80b15_00000]\n",
      "2025-07-24 15:12:29,418\tINFO tune.py:1041 -- Total run time: 24.54 seconds (24.52 seconds for the tuning loop).\n",
      "2025-07-24 15:12:29,498\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- SAC_ReachEnvGym_80b15_00000: FileNotFoundError('Could not fetch metrics for SAC_ReachEnvGym_80b15_00000: both result.json and progress.csv were not found at /home/ey/ray_results/SAC_2025-07-24_15-12-04/SAC_ReachEnvGym_80b15_00000_0_2025-07-24_15-12-04')\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# for _ in range(4):\n",
    "#     pprint(sac.train())\n",
    "from ray import train, tune\n",
    "\n",
    "# Create a Tuner instance to manage the trials.\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    # Specify a stopping criterion. Note that the criterion has to match one of the\n",
    "    # pretty printed result metrics from the results returned previously by\n",
    "    # ``.train()``. Also note that -1100 is not a good episode return for\n",
    "    # Pendulum-v1, we are using it here to shorten the experiment time.\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"env_runners/episode_return_mean\": 0.0},\n",
    "    ),\n",
    ")\n",
    "# Run the Tuner and capture the results.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train, tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    "    # Specify a simple tune hyperparameter sweep.\n",
    "    .training(\n",
    "        lr=tune.grid_search([0.001, 0.0005, 0.0001]),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a Tuner instance to manage the trials.\n",
    "tuner = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    # Specify a stopping criterion. Note that the criterion has to match one of the\n",
    "    # pretty printed result metrics from the results returned previously by\n",
    "    # ``.train()``. Also note that -1100 is not a good episode return for\n",
    "    # Pendulum-v1, we are using it here to shorten the experiment time.\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"env_runners/episode_return_mean\": -1100.0},\n",
    "    ),\n",
    ")\n",
    "# Run the Tuner and capture the results.\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlreach310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
